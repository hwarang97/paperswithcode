{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyNWpsViLzC/4JbVBi8QirRw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hwarang97/paperswithcode/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nugdvnAvrFZx",
        "outputId": "7b798751-32e0-4fce-b238-5f0c27e5e103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_model(depth):\n",
        "    if depth == 18:\n",
        "        return ResNet_18()\n",
        "\n",
        "    elif depth == 34:\n",
        "        return ResNet_34()\n",
        "\n",
        "    elif depth == 50:\n",
        "        return ResNet_50()\n",
        "\n",
        "    elif depth == 101:\n",
        "        return ResNet_101()\n",
        "\n",
        "    elif depth == 152:\n",
        "        return ResNet_152()\n",
        "\n",
        "    else:\n",
        "        print(f'check if model depth is in {[18, 34, 50, 101, 152]}')"
      ],
      "metadata": {
        "id": "-Bgn4hJiPBNf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Basic_Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.stride = stride\n",
        "        self.residual = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = self.residual(x)\n",
        "        if self.in_channels != self.out_channels or self.stride!=1:\n",
        "            identity = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, self.out_channels, kernel_size=1, stride=self.stride, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(self.out_channels)\n",
        "            )\n",
        "            shortcut = identity(x)\n",
        "        else:\n",
        "            shortcut = x\n",
        "        return self.relu(residual+shortcut)\n",
        "\n",
        "# class Identity_Block(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super().__init__()\n",
        "#         self.block = nn.Sequential(\n",
        "#             BasicConv2d(in_channels, in_channels),\n",
        "#             BasicConv2d(in_channels, in_channels)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "# class Identity_Block_L(nn.Module):\n",
        "#     def __init__(self, in_channels, ratio):\n",
        "#         super().__init__()\n",
        "#         self.block = nn.Sequential(\n",
        "#             BasicConv2d(in_channels, int(ratio*in_channels), kernel_size=1, padding=0), # 소수값이 들어가면 torch.empty() 함수에서 텐서를 생성할 때 에러 발생.\n",
        "#             BasicConv2d(int(ratio*in_channels), int(ratio*in_channels)),\n",
        "#             BasicConv2d(int(ratio*in_channels), 4*int(ratio*in_channels), kernel_size=1, padding=0)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "# class projection_Block(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super().__init__()\n",
        "#         self.block = nn.Sequential(\n",
        "#             BasicConv2d(in_channels, 2*in_channels, stride=2),\n",
        "#             BasicConv2d(2*in_channels, 2*in_channels)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "# class projection_Block_L(nn.Module):\n",
        "#     def __init__(self, in_channels):\n",
        "#         super().__init__()\n",
        "#         self.block = nn.Sequential(\n",
        "#             BasicConv2d(in_channels, int(0.5*in_channels), kernel_size=1, padding=0),\n",
        "#             BasicConv2d(int(0.5*in_channels), int(0.5*in_channels), stride=2),\n",
        "#             BasicConv2d(int(0.5*in_channels), 2*in_channels, kernel_size=1, padding=0)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.block(x)\n",
        "#         return x\n",
        "\n",
        "class ResNet_18(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.channels = 64\n",
        "\n",
        "        self.conv_2 = nn.Sequential(\n",
        "            *self.make_layer(2, self.channels, self.channels, stride=1)\n",
        "        )\n",
        "        self.conv_3 = nn.Sequential(\n",
        "            *self.make_layer(2, self.channels, 2*self.channels, stride=2)\n",
        "        )\n",
        "        self.conv_4 = nn.Sequential(\n",
        "            *self.make_layer(2, 2*self.channels, 4*self.channels, stride=2)\n",
        "        )\n",
        "        self.conv_5 = nn.Sequential(\n",
        "            *self.make_layer(2, 4*self.channels, 8*self.channels, stride=2)\n",
        "\n",
        "        )\n",
        "        self.gap = nn.AvgPool2d(7)\n",
        "        self.fc = nn.Linear(8*self.channels,1000)\n",
        "\n",
        "    def make_layer(self, num, in_channels, out_channels, stride):\n",
        "        layer = []\n",
        "        if in_channels != out_channels or stride!=1:\n",
        "            layer.append(Basic_Block(in_channels, out_channels, kernel_size=1, stride=stride))\n",
        "            for _ in range(num-1):\n",
        "                layer.append(Basic_Block(out_channels, out_channels))\n",
        "        else:\n",
        "            for _ in range(num):\n",
        "                layer.append(Basic_Block(in_channels, in_channels))\n",
        "        return layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_2(x)\n",
        "        x = self.conv_3(x)\n",
        "        x = self.conv_4(x)\n",
        "        x = self.conv_5(x)\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# class ResNet_34(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.channels = 64\n",
        "\n",
        "#         self.conv_2 = nn.Sequential(\n",
        "#             *self.make_layer(3, self.channels)\n",
        "#         )\n",
        "#         self.conv_3 = nn.Sequential(\n",
        "#             projection_Block(self.channels),\n",
        "#             *self.make_layer(3, 2*self.channels)\n",
        "#         )\n",
        "#         self.conv_4 = nn.Sequential(\n",
        "#             projection_Block(2*self.channels),\n",
        "#             *self.make_layer(5, 4*self.channels)\n",
        "#         )\n",
        "#         self.conv_5 = nn.Sequential(\n",
        "#             projection_Block(4*self.channels),\n",
        "#             *self.make_layer(2, 8*self.channels)\n",
        "#         )\n",
        "#         self.gap = nn.AvgPool2d(7)\n",
        "#         self.fc = nn.Linear(8*self.channels,1000)\n",
        "\n",
        "#     def make_layer(self, num, channels, Block):\n",
        "#         layer = []\n",
        "#         for _ in range(num):\n",
        "#             layer.append(Block(channels))\n",
        "#         return layer\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_2(x)\n",
        "#         x = self.conv_3(x)\n",
        "#         x = self.conv_4(x)\n",
        "#         x = self.conv_5(x)\n",
        "#         x = self.gap(x)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# class ResNet_50(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.channels = 64\n",
        "\n",
        "#         self.conv_2 = nn.Sequential(\n",
        "#             Identity_Block_L(self.channels, 1),\n",
        "#             *self.make_layer(2, 4*self.channels)\n",
        "#         )\n",
        "#         self.conv_3 = nn.Sequential(\n",
        "#             projection_Block_L(4*self.channels),\n",
        "#             *self.make_layer(3, 8*self.channels)\n",
        "#         )\n",
        "#         self.conv_4 = nn.Sequential(\n",
        "#             projection_Block_L(8*self.channels),\n",
        "#             *self.make_layer(5, 16*self.channels)\n",
        "#         )\n",
        "#         self.conv_5 = nn.Sequential(\n",
        "#             projection_Block_L(16*self.channels),\n",
        "#             *self.make_layer(2, 32*self.channels)\n",
        "#         )\n",
        "#         self.gap = nn.AvgPool2d(7)\n",
        "#         self.fc = nn.Linear(32*self.channels,1000)\n",
        "\n",
        "#     def make_layer(self, num, channels):\n",
        "#         layer = []\n",
        "#         for _ in range(num):\n",
        "#             layer.append(Identity_Block_L(channels, 0.25))\n",
        "#         return layer\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_2(x)\n",
        "#         x = self.conv_3(x)\n",
        "#         x = self.conv_4(x)\n",
        "#         x = self.conv_5(x)\n",
        "#         x = self.gap(x)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# class ResNet_101(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.channels = 64\n",
        "\n",
        "#         self.conv_2 = nn.Sequential(\n",
        "#             Identity_Block_L(self.channels, 1),\n",
        "#             *self.make_layer(2, 4*self.channels)\n",
        "#         )\n",
        "#         self.conv_3 = nn.Sequential(\n",
        "#             projection_Block_L(4*self.channels),\n",
        "#             *self.make_layer(3, 8*self.channels)\n",
        "#         )\n",
        "#         self.conv_4 = nn.Sequential(\n",
        "#             projection_Block_L(8*self.channels),\n",
        "#             *self.make_layer(22, 16*self.channels)\n",
        "#         )\n",
        "#         self.conv_5 = nn.Sequential(\n",
        "#             projection_Block_L(16*self.channels),\n",
        "#             *self.make_layer(2, 32*self.channels)\n",
        "#         )\n",
        "#         self.gap = nn.AvgPool2d(7)\n",
        "#         self.fc = nn.Linear(32*self.channels,1000)\n",
        "\n",
        "#     def make_layer(self, num, channels):\n",
        "#         layer = []\n",
        "#         for _ in range(num):\n",
        "#             layer.append(Identity_Block_L(channels, 0.25))\n",
        "#         return layer\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_2(x)\n",
        "#         x = self.conv_3(x)\n",
        "#         x = self.conv_4(x)\n",
        "#         x = self.conv_5(x)\n",
        "#         x = self.gap(x)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.fc(x)\n",
        "#         return x\n",
        "\n",
        "# class ResNet_152(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.channels = 64\n",
        "\n",
        "#         self.conv_2 = nn.Sequential(\n",
        "#             Identity_Block_L(self.channels, 1),\n",
        "#             *self.make_layer(2, 4*self.channels)\n",
        "#         )\n",
        "#         self.conv_3 = nn.Sequential(\n",
        "#             projection_Block_L(4*self.channels),\n",
        "#             *self.make_layer(7, 8*self.channels)\n",
        "#         )\n",
        "#         self.conv_4 = nn.Sequential(\n",
        "#             projection_Block_L(8*self.channels),\n",
        "#             *self.make_layer(35, 16*self.channels)\n",
        "#         )\n",
        "#         self.conv_5 = nn.Sequential(\n",
        "#             projection_Block_L(16*self.channels),\n",
        "#             *self.make_layer(2, 32*self.channels)\n",
        "#         )\n",
        "#         self.gap = nn.AvgPool2d(7)\n",
        "#         self.fc = nn.Linear(32*self.channels,1000)\n",
        "\n",
        "#     def make_layer(self, num, channels):\n",
        "#         layer = []\n",
        "#         for _ in range(num):\n",
        "#             layer.append(Identity_Block_L(channels, 0.25))\n",
        "#         return layer\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.conv_2(x)\n",
        "#         x = self.conv_3(x)\n",
        "#         x = self.conv_4(x)\n",
        "#         x = self.conv_5(x)\n",
        "#         x = self.gap(x)\n",
        "#         x = torch.flatten(x, start_dim=1)\n",
        "#         x = self.fc(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "2D-Ks0kkrzjp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "depth = 18 # [18, 34, 50, 101, 152]\n",
        "model = select_model(depth)\n",
        "model.train()\n",
        "\n",
        "summary(model, input_size=(10,64,56,56), device='cpu') # 기본적으로 gpu에 입력값을 생성하기에, cpu로 변경시켜줘야함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZmKr9Sv5Vi8",
        "outputId": "c26009a2-2664-4953-f661-a48d4b44136b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "ResNet_18                                [10, 1000]                --\n",
              "├─Sequential: 1-1                        [10, 64, 56, 56]          --\n",
              "│    └─Basic_Block: 2-1                  [10, 64, 56, 56]          --\n",
              "│    │    └─Sequential: 3-1              [10, 64, 56, 56]          73,984\n",
              "│    │    └─ReLU: 3-2                    [10, 64, 56, 56]          --\n",
              "│    └─Basic_Block: 2-2                  [10, 64, 56, 56]          --\n",
              "│    │    └─Sequential: 3-3              [10, 64, 56, 56]          73,984\n",
              "│    │    └─ReLU: 3-4                    [10, 64, 56, 56]          --\n",
              "├─Sequential: 1-2                        [10, 128, 29, 29]         --\n",
              "│    └─Basic_Block: 2-3                  [10, 128, 29, 29]         --\n",
              "│    │    └─Sequential: 3-5              [10, 128, 29, 29]         156,160\n",
              "│    │    └─ReLU: 3-6                    [10, 128, 29, 29]         --\n",
              "│    └─Basic_Block: 2-4                  [10, 128, 29, 29]         --\n",
              "│    │    └─Sequential: 3-7              [10, 128, 29, 29]         295,424\n",
              "│    │    └─ReLU: 3-8                    [10, 128, 29, 29]         --\n",
              "├─Sequential: 1-3                        [10, 256, 16, 16]         --\n",
              "│    └─Basic_Block: 2-5                  [10, 256, 16, 16]         --\n",
              "│    │    └─Sequential: 3-9              [10, 256, 16, 16]         623,616\n",
              "│    │    └─ReLU: 3-10                   [10, 256, 16, 16]         --\n",
              "│    └─Basic_Block: 2-6                  [10, 256, 16, 16]         --\n",
              "│    │    └─Sequential: 3-11             [10, 256, 16, 16]         1,180,672\n",
              "│    │    └─ReLU: 3-12                   [10, 256, 16, 16]         --\n",
              "├─Sequential: 1-4                        [10, 512, 9, 9]           --\n",
              "│    └─Basic_Block: 2-7                  [10, 512, 9, 9]           --\n",
              "│    │    └─Sequential: 3-13             [10, 512, 9, 9]           2,492,416\n",
              "│    │    └─ReLU: 3-14                   [10, 512, 9, 9]           --\n",
              "│    └─Basic_Block: 2-8                  [10, 512, 9, 9]           --\n",
              "│    │    └─Sequential: 3-15             [10, 512, 9, 9]           4,720,640\n",
              "│    │    └─ReLU: 3-16                   [10, 512, 9, 9]           --\n",
              "├─AvgPool2d: 1-5                         [10, 512, 1, 1]           --\n",
              "├─Linear: 1-6                            [10, 1000]                513,000\n",
              "==========================================================================================\n",
              "Total params: 10,129,896\n",
              "Trainable params: 10,129,896\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 18.87\n",
              "==========================================================================================\n",
              "Input size (MB): 8.03\n",
              "Forward/backward pass size (MB): 265.91\n",
              "Params size (MB): 40.52\n",
              "Estimated Total Size (MB): 314.46\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}